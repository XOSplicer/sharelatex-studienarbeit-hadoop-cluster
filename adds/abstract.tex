% !TEX root = ../master.tex
\chapter*{Abstract}

\begingroup
  \begin{table}[h!]
    \setlength\tabcolsep{0pt}
    \begin{tabular}{p{3.5cm}p{10.0cm}}
      Title & \dertitel \\
      Author: & \derautor \\
    \end{tabular}
  \end{table}
\endgroup

\hspace{2cm}

The ever-increasing amount of data available in the current digital age 
poses major challenges, 
a phenomenon widely known as \emph{Big Data}. 
In order to gain insight from this data,
one needs to have information systems in place that 
provide a platform to analyze this data in an efficient manner.

The computing power of single computers is limited by both physical 
as well as economic constraints,
hence a multitude of machines can be connected to form \emph{computing clusters}
that are capable of performing data analysis on Big Data scale.
In order to run such a cluster so-called \emph{cluster managers} are needed 
to organize the execution of task and distribution of data among the cluster nodes.

A popular cluster manager for Big Data clusters in the industry is the \emph{Hadoop} framework. 
Hadoop offers distributed storage for arbitrary-sized collections of data 
by combining the individual computing and storage resources of commodity hardware. 
Furthermore, it provides programming interfaces for analyzing the stored data.

This research project is embedded into a larger body of investigation 
regarding the use of Hadoop at the \acf{DHBW} in both research and training. 
Thus, this work specifically deals with the following topics:

\begin{itemize}
    \item Examination of existing infrastructure and its capability to host a Hadoop cluster
    \item Deployment of a Hadoop cluster. 
    This also includes the exploration of possibilities for automatic configuration 
    and installation of such a cluster.
\end{itemize}

As an outcome of this project, 
a detailed execution plan to perform the deployment is given.
Furthermore the deployment is rolled out to the given environment. 

%TODO: Update after project


