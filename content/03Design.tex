% !TEX root = ../master.tex
\chapter{Design}
\label{chap:design}

\section{Analysis of Cluster Environment}

The Hadoop cluster will be installed on the cluster computer at the \ac{DHBW} Mannheim.
This cluster utilizes \emph{OpenStack} \urlinline{https://www.openstack.org/} to provide resources to its users. 
OpenStack is an open source project to manage cloud computing architectures.
It enables the compositions of compute elements (virtual machines), network elements (sub-nets and routers), 
and storage elements (virtual disks) to form a virtual computing environment.
The OpenStack management web interface can be accesses at (TODO check url)\urlinline{https://controller.c4.dhbw-mannheim.de/}. 
The \emph{Cloud Computing Competence Center} at the \ac{DHBW} Mannheim  manages the cluster computer.

The OpenStack environment is shared between multiple user groups, 
which each is given a certain amount of resources. 
For this project the student group is given a limited amount of resources.
(TODO insert how many of each unit(20 vcpu, 50 gb ram, 1TB storage))
This resources can be used to allocate instances of \acp{VM} in pre-defined units and assign virtual disks to them.
The \acp{VM} can be connected to the network of the \ac{DHBW} and can be interconnected using internal, virtual networks.
Firewall restrictions can be set through group policies, which can be assigned to the \acp{VM}. 
When creating a new \ac{VM} most of these settings must be supplied. 
Furthermore the operating system flavour can be chosen and a \ac{SSH} login key can be provided.
Among others \emph{Ubuntu 16.04 LTS} can be used.

The Hadoop cluster be deployed on multiple interconnected \acp{VM} in the OpenStack environment that can be accessed from the university network.

The connection to the \ac{DHBW} network gives some restrictions: \acp{VM} can only be accessed from the university network and not from the Internet.
From a security point of view this can be considered good. However for development access it is quite unfortunate. 


\section{Analysis of Requirements}

There are three main user groups that can be determined for the Hadoop cluster:

\begin{itemize}
    \item \textbf{IT Staff} -- The technical staff at the \ac{DHBW} can perform maintenance on the Hadoop cluster and possibly a re-setup of it.
    Also they might use it for research projects.
    \item \textbf{Lecturers} -- Lecturers might most likely use the cluster for the purpose of teaching classes about \ac{BDA} and give hands on experiences for the students. They might prepare the educational material and data sets on the cluster and exemplary demonstrate its usage.
    \item \textbf{Students} -- Students at the \ac{DHBW} might use the cluster within the context of courses or research project. Within lectures they might be instructed by the 
    
    TODO use in lectures and projects. maybe setup for ed. purposes
\end{itemize}

In his paper, \emph{Philipp Winter} discusses more possible use cases for the cluster. It focuses on the educational usability in lectures.
(TODO write about this paper, what is it really)
Main users are lecturers and students to perform analytics of various data on the cluster.

Sicklinger et al. also did a requirements analysis for an Hadoop cluster at the \ac{DHBW}. 
\begin{itemize}
    \item The cluster utilizes the OpenStack Environment in Mannheim.
    \item There should be only a low cost, at best no cost, associated with deploying and running the cluster.
    \item It should be fast and easy for students and lecturers to deploy the cluster.
    \item The cluster should provide the ability to be used to learn about the Hadoop deployment and learn about data analysis.
    \item The it should support recent tools to be used alongside with Hadoop.
    \item The cluster should scale to the needs of the users.
\end{itemize}
\autocite[][p. 40ff]{wi2018managementsystems}


All of the listed requirements imply that the cluster needs to be easy to deploy. 
Automated deployment is therefore favourable to make the procedure reproducible. 


\section{Research on Possibilities}

After the requirements are settled,
the possibilities of how Hadoop can be installed 
on the given infrastructure will be researched.
This will lead to an decision on the method that should be used for the cluster at the \ac{DHBW}.

\subsubsection{Cluster Management Systems}

To deploy Hadoop on an cluster there are two general approaches. Either the Hadoop distribution package is installed and configured manually on each node or an management tool is used to perform automatic deployment and configuration. 
Since the manual installation process is error prone (as we will see later) and needs some know-how,
it is  more easy for the user to utilize an management system for the cluster.

The management system can provide, install, manage and monitor software components on the cluster nodes.
Not only \ac{HDFS} and \ac{YARN} can be deployed,
but also additional tools can be integrated into the cluster this way.
Usually the management system also provides an graphical user interface to perform those tasks.

To provide the components the management system downloads them from a given binary distribution repository. The installation can be performed via remote access from the management server to the nodes. Monitoring gives valuable information to the cluster user, such as capacity an utilization of computational power and storage.

Some vendors choose to bundle a management system 
together with an specific version of Hadoop and additional Tools. They offer this bundle mostly as binary packages which are considered \emph{distributions} of Hadoop.
For system maintainers this holds the benefit that no compilation from source is necessary and the versions of the tools are adjusted to work with each other. 
Notable distributions are \emph{\acf{CDH}} and \emph{\acf{HDP}}. 

There are four ways to install Hadoop that are considered here:

\begin{itemize}
    \item A \emph{manual installation} of Hadoop 
        and every additional piece of software from the ecosystem
    \item \emph{\acf{CDH}} --  An open source software distribution by Cloudera including Hadoop
        and additional software from the ecosystem
        \urlinline{https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html}
    \item \emph{Ambari} -- An open source project by Apache that provides
        the capability to manage Hadoop Clusters and the software components running on it.
        \urlinline{https://ambari.apache.org/}
    \item \emph{\acf{HDP}} -- Hortonswork's ready-to-install distribution of Hadoop 
        with Ambari and additional software
        \urlinline{https://de.hortonworks.com/products/data-platforms/hdp/}
        
\end{itemize}

TODO References

Since \ac{HDP} makes use of Ambari for installing the cluster and managing the installed software while it simply provides a pre-packaged version of the components, 
it can be preferred over using Ambari directly which would create the need to compile it from source.


\subsubsection{Evaluation Environment}
In the coming sections each of those three resulting possibilities will be looked at in more detail.
For each of the options an evaluation will be made on whether it is 
feasible to deploy an Hadoop installation with the methods that they provide.
Therefore a simple test installation in an OpenStack environment is made.
For this purpose the OpenStack environment \emph{BWCloud} \urlinline{https://www.bw-cloud.org/de/projekt}
is used, an environment that is available for students at the \ac{DHBW} 
and other universities in Baden-WÃ¼rttemberg for educational purposes. 
It is generally an equivalent software to the actual cluster environment, 
however it is accessible from the Internet, 
which makes it more comfortable to work with while performing the tests.

\subsection{Manual Installation of Hadoop}

The obvious first possibility to install Hadoop onto a number of hosts 
is to use the installation package provided by Apache themself \urlinline{https://hadoop.apache.org/releases.html}.
This represents the most basic procedure of installation.
Tom White \autocite[][Appendix A]{white2015hadoop} describes how Hadoop can be manually installed on the machines in an cluster. 
Manually here means without the help of automated installers for a single machine or the complete cluster. 
Every aspect of the installation and configuration must be done \enquote{by hand}.
White gives step by step instruction on the process.

Since no step in this step is predetermined or automated, this method does not really intall an "Hadoop distribution" on the cluster, but rather only \ac{YARN}, \ac{HDFS} and MapReduce.
It does not provide an \emph{management system} for Hadoop.

\subsubsection{System Setup}
For the installation two \acs{VM} are used, utilizing two virtual \ac{CPU} cores and eight \ac{GB} \ac{RAM} and a 20 \ac{GB} storage drive. 
Ubuntu Server \urlinline{https://www.ubuntu.com/server} 16.04.3 \ac{LTS} is used as the operating system. 
The machines are connected to the public Internet and to each other using two network interfaces each.

The two machines are going to be connected together in an master-slave setup
where the master runs the \ac{YARN} resource manager and the \ac{HDFS} name node as well as an \ac{HDFS} data node and an \ac{YARN} client node.
The slaves runs only an \ac{HDFS} data node and an \ac{YARN} client node.

First the system of the master is configured. Therefore the following steps are taken.

\begin{enumerate}
    \item The system is updated and OpenJDK 1.8 \urlinline{http://openjdk.java.net/}  is installed using \texttt{apt}. 
    \item A new user \emph{hadoop} is created which will be used to run every service on the machine concerning Hadoop.
    \item For this user a new \ac{SSH} key is created which enables password-less access to the two machines for the user \emph{hadoop} if its public key is later added to the \texttt{authorized\_key} file of the user on the hosts.
    \item The \texttt{/etc/hosts} file is adjusted, such that both hosts know how to reach each other on the local network by using their \acs{FQDN} which is resolved to an \ac{IP} address.
    \item The network interface to the internal network that is connected is configured to use the \ac{IP} address assigned to it by OpenStack.
    \item \ac{IP}v6 is disabled.
\end{enumerate}

The slave system uses mainly the same configuration but is altered slightly.
No new \ac{SSH} key is generated, but the public key of the previously
generated one is entered into the \texttt{authorized\_key} file. 
Furthermore an additional 70 \ac{GB} hard drive is attached 
and mounted to store working data.

After the base system is configured, Hadoop can be installed on both machines
and can be configured to form a cluster.
Again, first the master host is configured.
Therefore the next steps describes in 
\autocite[][Appendix A]{white2015hadoop}
can be followed.

\begin{enumerate}
    \item First Hadoop is downloaded from the Apache website \urlinline{https://hadoop.apache.org/releases.html}. 
    At the moment of performing this tests, 
    version 2.7.4 is the latest stable release.
    The downloaded packed file is unzipped in the home directory of the \emph{hadoop} user.
    \item Inside the \texttt{bashrc} file of the user the the variables \texttt{JAVA\_HOME} is pointing to the OpenJDK installation,
    \texttt{HADOOP\_HOME} is set to the folder that has just been crated and the \texttt{PATH} variable is configured to include the \texttt{bin} directory within \texttt{HADOOP\_HOME}.
    Each variable is exported to be available in the user shell.
    This makes Hadoop usable from the login shell of the user.
    \item Now the configuration files of Hadoop are adjusted to the cluster needs. Each of those resides in the \texttt{etc/hadoop} directory within \texttt{HADOOP\_HOME}.
    \begin{itemize}
        \item \texttt{core-site.xml} -- This file describes the most basic settings for the instaled Hadoop instance. 
        Here the settings for the file system is set to use the \ac{FQDN} of the master to connect to.
        \item \texttt{hadoop-env.sh} -- This file describes all the environment variables used while running Hadoop. 
        Here \texttt{JAVA\_HOME} is set to the same value used above.
        \item \texttt{hdfs-site.xml} This file describes how \ac{HDFS} is configured. 
        The storage directory is set to a persistent folder
        and \ac{HDFS} is instructed to bind to the \ac{IP} address 0.0.0.0 
        which means the file system is accessible from all network interfaces.
        \item \texttt{mapred-site.xml} -- Here the settings for MapReduce are made. 
        It is instructed to use \ac{YARN} as the underlying framework.
        \item \texttt{slaves} -- This file is used on the master host to identify all the hosts 
        that run an nodemanager or an datanode.
        Therefore the \acs{FQDN} of the master and the salve host are entered here.
        \item \texttt{yarn-site.xml} -- This file describes the settings for \ac{YARN}. Here the master's \ac{FQDN} is entered as the resourcemanager and \ac{YARN} is bound to 0.0.0.0. 
        Furthermore the minimal allocation units for \ac{CPU} cores 
        as well as \ac{RAM} and their respective maximum allocation are defined. 
        This dictates how \ac{YARN} uses the resources of this host.
    \end{itemize}
\end{enumerate}

After the configuration is done for the master host, the same procedure can be followed for the slave. 
The only exceptions are that no entries are made in the \texttt{slaves} file 
and the \texttt{hdfs-site.xml} is configured, so that \ac{HDFS} uses the attached data drive to store file contents.

Finally the \ac{HDFS} file systems on each host can be formatted using the TODO WHICH COMMAND command.
Then \ac{YARN} and \ac{HDFS}, aswell as the MapReduce history server is started on the master, 
which in turn start the namenode and resoucemanager on the master 
as well as the datanoode and nodemanager on both the master and the slave.
Now the cluster can be used to store data in \ac{HDFS} and run MapReduce jobs.

\subsubsection{Tests}
\autocite[][]{white2015hadoop} provides an online reference (http://hadoopbook.com/)
to test data and MapReduce programs 
that can be used for demonstration purposes of the Hadoop cluster.
The provided data includes weather data produced by the \ac{NCDC} and programs that can be run on Hadoop to analyze this data. 
Storing this data in \ac{HDFS} and running the MapReduce jobs lead to the discovery of erroneous configurations in the cluster.

Further test included the execution \emph{teragen} and \emph{terasort}, also provided by White, which generate a variable amount of numbers and sorts them, in this case 5 \ac{GB} of data. 
The generation  was successful, however the sorting failed due to an repeated connection timeout, which could not be resolved.

\subsubsection{Errors and Lessons Learned}
The whole described setup procedure is performed manually 
and therefore rather error prone.
Special attention to the usage of \acs{FQDN} must be given, 
so that the names used in Hadoop and the hostnames match up.
Those also need to be used in the \texttt{hosts} files on all hosts.

Furthermore it is important to configure the firewalls of OpenStack correctly, 
so that the nodes can be reached internally and externally.

Errors in the configuration lead to nodes that are not recognized by Hadoop or jobs that do not succeed.
It is rather frustrating to find and fix those issues for the person installing the cluster.

\subsubsection{Conclusions}

Due to the difficulties that the method of manually installing Hadoop on each host holds it is favorable to use a more streamlined 
and automated installation method as described in the next two sections.
This way a lot of manual configuration can be avoided.
Furthermore integrating tools such as Spark into the cluster 
will lead to even more complex installation and configuration processes which again might introduce new openings for errors.

\subsection{\acl{HDP} with Ambari deployed with Ansible}
TODO describe HDP, Ambari, Ansible

\subsubsection{System Setup}

As in the sections before the OpenStack BW-Cloud is used for a test deployment.
Three \acs{VM} are started, each with 20 \ac{GB} disk space, Ubuntu 16.04 \ac{LTS}, an public \ac{IP} address. 
The hosts are connected with an internal \ac{IP} network.
One \ac{VM} are used as the Ambari and Hadoop master node, having 2 virtual \ac{CPU} cores and 4 \ac{GB} \ac{RAM}.
Two \acs{VM} are used as slave nodes with 4 \acs{CPU} and 8 \ac{GB} \ac{RAM} as well as 2 \acs{CPU} and 4 \ac{GB} \ac{RAM} respectively.

Ansible is used to perform all system configuration. 
However before Ansible can be used, Python 2.7 needs to be installed on the hosts.
Now every deployed configuration is reproducible by running the Ansible configuration management again.
Hortonworks describes which steps of system setup are necessary 
to run Ambari when the installation is performed on a shell \autocite[][]{hortonworks2018install}.
The configuration using Ansible  performs the following similar steps:
\begin{enumerate}
    \item The internal network interfaces are configured 
        and the \texttt{\/etc\/hosts} file is filled with the hostnames 
        of all nodes and their integral \acs{IP}. 
        This prevents issues with the \ac{FQDN} resolution.
    \item The \ac{NTP} service is installed and started. 
        This is a requirement for Ambari.
    \item The \ac{THP}, a Linux kernel feature to improve memory look-ups, 
        is disabled using a service script. 
        This is a requirement for Ambari, 
        as it would reduce database performance.
        The script is based on \autocite{braun2017hugepages}.
    \item The Hortonworks Ambari repositories are added, so that the provided     binaries can be used.
    \item On the master \ac{VM} the Ambari server 
        and the Ambari agent is installed.
    \item On the slave nodes only the Ambari agent is installed.
    \item On the slave node the Ambari agent configuration is set to use the
        master node as its server.
    \item On the master node the initial Ambari server setup is performed which
        installs an \ac{JDK} 
        and starts the Ambari server service as the root user.
\end{enumerate}

Now the Hadoop cluster can be deployed using the Ambari Cluster Manager 
via an web interface on port 8080 on the master node, 
which needs to be accessible through the firewall of OpenStack.
Ambari provides an install wizard for the cluster installation. 
Here a new cluster deployment counting all three hosts is started.
The node need to be registered manually, as no shared \ac{SSH} key is present, that allows root access to the machines. 
(The web interface would require the used to upload the private key for root access, which is generally unsafe.)
The master node is configured to act as an Hadoop master and salve an as the master for all recommended services running alongside of Hadoop.
The slaves are salves for \ac{HDFS} and \ac{YARN}.
The wizards now installs the cluster.


\subsection{Errors and Lessons Learned}
\label{sec:ambari:errors}
Since a lot of services such as Spark are recommended to be installed, 
the 20 \ac{GB} disk of the master is filled completely and the installation of the cluster fails. At least 50 \ac{GB} are recommended for an correct installation.
Therefore thee installation fails and the Hadoop deployment can not be functionally tested for now.

Ambari needs to be provided with the \ac{FQDN} of an host 
in all forms and configuration files. 
Otherwise it can not detect the agents or server.

The Ambari clients need to be configured 
to use the correct Ambari server host.
Otherwise they will not be detected in the manual registration.


\subsubsection{Conclusions}
Ansible provides an reproducible way of system configuration 
and can therefore be used to install Ambari, 
which otherwise would require error-prone manual configuration on each host.
The Ambari Cluster Manager can then be used to install Hadoop and other services on the cluster with an user friendly web interface.


\subsection{\acl{CDH}}

Cloudera distributes an Hadoop management system called \ac{CDH}.
The company provides an docker image which can be used to easily evaluate the usability of \ac{CDH}.
It contains \enquote{a single-host deployment of the Cloudera open-source distribution, including CDH and Cloudera Manager.} \autocite[][]{cloudera2018docker}
It does not provide a full cluster installation, but only a test environment to evaluate the plattform.

TODO explain what Cloudera distribution does


\subsubsection{System Setup}

As in the previous section, BW-Cloud will be used for installing a test setup.
Since the Docker image only runs on a single host, this host should be as powerfull as possible.
Therefore the largest available \ac{VM} preset is used, which consistes of 2 virtual \ac{CPU} cores, 
4 \ac{GB} \ac{RAM}, and 20 \ac{GB} storage. The operating is again Ubuntu 16.04 \ac{LTS}, 
and the host is accessible via the Internet. 
Cloudera describes how Docker can be installed and the test image can be started, 
these instructions are followed. \autocite[][]{cloudera2018docker}
To do so, the Docker runtime is installed, a new  user \emph{docker} is created and added to the \emph{docker} group. To test the Docker installation, the \emph{hello-world} image can be started.
Now all ports required by \ac{CDH} need to be accesible through the OpenStack firewall.
Namely these are 8888, 7080, and 8080 (which is used to redirect to port 80 inside the container).
After fetching the Docker  image, it can be started in daemon mode with the necessary ports exposed using the command\\
\texttt{\$ docker run --hostname=quickstart.cloudera --privileged=true -t -i -d -p 8888 -p 7080 -p 8080:80 cloudera/quickstart /usr/bin/docker-quickstart}\\
All services that are needed inside the container are started automatically.


\subsubsection{Tests}
The Docker image includes an web based tutorial, 
that can be accessed on port 8080 (remapped before) on the host.
The tutorial highlights the features of \ac{CDH} which differ from other Hadoop distributions.
The tutorial describes a story from the perspective of an data analyst and shows how the features provide value for typical use cases.

\subsubsection{Conclusions}
The tutorial provides a zero-configuration hands-on experience which focuses on the features specific to Cloudera's Hadoop distribution.
The installation inside the docker container seems stable.
However it is not possible to assume that this implies an error free installation on an cluster, 
since possible obstacles are not foreseeable.

\subsection{Decision on Distribution}
\label{sec:decision}

At this point it is important to mention the project paper by Sicklinger et al., 
who did research on the topic of the possible usage of Hadoop management systems for the \ac{DHBW}.
Their findings suggest using Apache Ambari to deploy an Hadoop cluster at the \ac{DHBW}.
According to them, installing Hadoop manually is ruled out due to the technical complexity.
Apache Ambari and Cloudera are equally suitable for the usage at the \ac{DHBW}, 
but Ambari should be favoured since it is free to use and crates no licensing costs.
\autocite[][p. 53f]{wi2018managementsystems}

In combination with Ansible, Ambari ensures an reproducible cluster installation. 
Therefore Ambari is the solution that will be used for this project and Ansible will be used to install the \ac{HDP} including Ambari.

\section{Architecture Design}

For the deployment of the Hadoop cluster, OpenStack will be used.
It provides the possibilities to create computational resources, which are \acs{VM}, 
storage resources, which are virtual disk volumes, 
and network resources, which are either project internal \ac{IP} networks or networks that are accessible from the outside of OpenStack.
With these resources it is possible to create an cluster that consists of interconnected nodes, 
that can store and process data and be accessed from the outside.

For this project a master-slave architecture is chosen.
Ambari promotes a sever-agent architecture, 
where the user can interact with Ambari on the server host 
and deploy the Hadoop cluster to all agent hosts.
Hadoop uses an cluster setup in which one server distributes and manages \ac{YARN} jobs (the \ac{YARN} resourcemanager), 
and keeps track of where data is stored data \ac{HDFS} (the \ac{HDFS} namenode).
This can be considered a master.
To increase availability it is possible to choose a secondary master which replicates the \ac{HDFS} namenode.
Figure \vref{fig:architecture} depicts the architecture that follows from these prerequisites.

\begin{figure}
	\fbox{
	    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{resources/architecture.pdf}
    }
	\caption{\label{fig:architecture}Proposed Architecture}
\end{figure}

As discovered in section \vref{sec:ambari:errors} there should be at least 50 \ac{GB} of disk space on each node, 
just for Ambari and Hadoop to be installed.
For \ac{HDFS} data storage an additional volume should be attached to each of the \ac{HDFS} datanodes.

The size of the used \acs{VM} should be the one with the most memory, 
as this is the resource where the overhead of having multiple \acs{VM} has the most impact.
All nodes need access to the Internet to download software packages and should be accessible via \ac{SSH} through the OpenStack firewall.
Furthermore the master node needs to be accessible via typical ports for the web interface of the deployed services.
Internally all nodes need to be connected via an private network with fixed \acs{IP}, where the mapping between hostnames and \acs{IP} is known for each host. Typically this can be done with \ac{DNS}, however it is out of scope for this project to setup an \ac{DNS} server. Filling the \texttt{\/etc\/hosts} file is sufficient.
The internal network should not be restricted by any firewall.

Tom White recommends the following specification for a single node in a cluster, based on the needs of a full \ac{BDA} system in 2014:\\
Each node should run on dedicated, commodity hardware.
\begin{itemize}
    \item Processor - Two hex/octo-core 3 GHz \acs{CPU}
    \item Memory 64-512 \ac{GB} \ac{ECC} \ac{RAM}
    \item Storage - 12 to 24 times 1 to 4 \ac{TB} disks
    \item Network - Gigabit Ethernet with link aggregation
\end{itemize}
\autocite{white2015hadoop}

However the given OpenStack environment does not allow the allocation of such amounts of resources. Therefore the maximum sized \acs{VM} should be used, and each be given an equal share of the available storage.

\section{Execution Plan}

In chapter \ref{chap:impl} the Hadoop cluster will be set up in an automated way.
To do so an execution plan is provided.
It functions as a reference to follow along during the implementation 
and can be used to reproduce the results in a similar way, i.e. to set up the cluster again in an similar environment.


In section \vref{sec:decision} the decision to use Apache Ambari in combination with Ansible has been made.
This scenario leads to the following steps regarding the deployment of the cluster into the OpenStack environment:

\begin{enumerate}
    \item \textbf{\ac{VM} Creation in OpenStack} TODO Fist create inner network and security groups, then appropriate number of VMs according to \label{fig:architecture}, at least 3, names accordingly. ATTENTION: Network connection: due to a bug in openstack first only outer network connection then attach inner connection and reboot. note down the internal and external ips displayed by openstack, note down the eth port that is newly created for internal connection. (maybe use enumeration here and in other items)
    \item \textbf{System Set-Up with Ansible} ("must still be implemened")TODO  see \urlinline{https://github.com/XOSplicer/studienarbeit-hadoop-cluster-ansible},
    therefore install dependency roles, configure ssh\_config and ansible.cfg (UPDATE README) use the ips and hostnames from the before. (in implementation explain how the ansible part works). finally role out the configuration by running ansible command
    \item \textbf{Hadoop Set-Up with Ambari} TODO with gui installer sinply follow instructions, select minimal amout of components possible or the ones recommended by jonas. when promted to select nodes use manual registaration: reason dont upload ssh private key for root access (maybe in chpt 4). also configure each service close to default with a common password (how to document the password?)
    \item \textbf{Test Usage} TODO test HDFS in general by storing some data. test map reduce by running teragen / terasort from Hadoop Book (Tom White)
\end{enumerate}

TODO
