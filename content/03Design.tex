% !TEX root = ../master.tex
\chapter{Design and Methodology}
\label{chap:design}

\section{Analysis of Cluster Environment and Requirements}

TODO

\section{Research on Possibilities}

After the requirements are settled,
the possibilities of how Hadoop can actually be installed 
on the given infrastructure will be researched.

TODO

TODO explain what an distribution / managemnt system is

There are four ways to install Hadoop that are considered here:

\begin{itemize}
    \item A \emph{manual installation} of Hadoop 
        and every additional piece of software from the ecosystem
    \item \emph{\ac{CDH}} --  An open source software distribution by Cloudera including Hadoop
        and additional software from the ecosystem
    \item \emph{Ambari} -- An open source project by Apache that provides
        the capability to manage Hadoop Clusters and the software components running on it.
    \item \emph{\ac{HDP}} -- Hortonswork's ready-to-install distribution of Hadoop 
        with Ambari and additional software
\end{itemize}

TODO References

Since \ac{HDP} makes use of Ambari for installing the cluster and managing the installed software while it simply provides a pre-packaged version of the components, 
it can be preferred over using Ambari directly which would create the need to compile it from source.

In the coming sections each of those three resulting possibilities will be looked at in more detail.
For each of the options an evaluation will be made on whether it is 
feasible to deploy an Hadoop installation with the methods that they provide.
Therefore a simple test installation in an OpenStack environment is made.
For this purpose the OpenStack environment \emph{BWCloud}\footnote{https://www.bw-cloud.org/de/projekt} 
is used, an environment that is available for students at the \ac{DHBW} 
and other universities in Baden-WÃ¼rttemberg for educational purposes. 
It is generally an equivalent software to the actual cluster environment, 
however it is accessible from the internet, 
which makes it more comfortable to work with while performing the tests.

\subsection{Manual Installation of Hadoop}

The obvious and naive first possibility to install Hadoop onto a number of hosts 
is to use the installation package provided by Apache themself\footnote{https://hadoop.apache.org/releases.html}.
This represents the most basic procedure of installation.
Tom White \autocite[][Appendix A]{white2015hadoop} describes how Hadoop can be manually installed on the machines in an cluster. 
Manually here means without the help of automated installers for a single machine or the complete cluster. 
Every aspect of the installation and configuration must be done \enquote{by hand}.
White gives step by step instruction on the process.

Since no step in this step is predetermined or automated, this method does not really intall an "Hadoop distribution" on the cluster, but rather only \ac{YARN}, \ac{HDFS} and MapReduce.
It does not provide an \emph{management system} for hadoop.

\subsubsection{System Setup}
For the installation two \acs{VM} are used, utilizing two virtual \ac{CPU} cores and eight \ac{GB} \ac{RAM} and a 20 \ac{GB} storage drive. 
Ubuntu Server \footnote{https://www.ubuntu.com/server} 16.04.3 \ac{LTS} is used as the operating system. 
The machines are connected to the public internet and to each other using two network interfaces each.

The two machines are going to be connected together in an master-slave setup
where the master runs the \ac{YARN} resource manager and the \ac{HDFS} name node as well as an \ac{HDFS} data node and an \ac{YARN} client node.
The slaves runs only an \ac{HDFS} data node and an \ac{YARN} client node.

First the system of the master is configured. Therefore the following steps are taken.

\begin{enumerate}
    \item The system is updated and OpenJDK 1.8\footnote{http://openjdk.java.net/}  is installed using\texttt{apt}. 
    \item A new user \emph{hadoop} is created which will be used to run every service on the machine concerning Hadoop.
    \item For this user a new \ac{SSH} key is created which enables password-less access to the two machines for the user \emph{hadoop} if its public key is later added to the \texttt{authorized\_key} file of the user on the hosts.
    \item The \texttt{/etc/hosts} file is adjusted, such that both hosts know how to reach each other on the local network by using their \acs{FQDN} which is resolved to an \ac{IP} address.
    \item The network interface to the internal network that is connected is configured to use the \ac{IP} address assigned to it by OpenStack.
    \item \ac{IP}v6 is disabled.
\end{enumerate}

The slave system uses mainly the same configuration but is altered slightly.
No new \ac{SSH} key is generated, but the public key of the previously
generated one is entered into the \texttt{authorized\_key} file. 
Furthermore an additional 70 \ac{GB} hard drive is attached 
and mounted to store working data.

After the base system is configured, Hadoop can be installed on both machines
and can be configured to form a cluster.
Again, first the master host is configured.
Therefore the next steps describes in 
\autocite[][Appendix A]{white2015hadoop}
can be followed.

\begin{enumerate}
    \item First Hadoop is downloaded from the Apache website\footnote{https://hadoop.apache.org/releases.html}. 
    At the moment of performing this tests, 
    version 2.7.4 is the latest stable release.
    The downloaded packed file is unzipped in the home directory of the \emph{hadoop} user.
    \item Inside the \texttt{bashrc} file of the user the the variables \texttt{JAVA\_HOME} is pointing to the OpenJDK installation,
    \texttt{HADOOP\_HOME} is set to the folder that has just been crated and the \texttt{PATH} variable is configured to include the \texttt{bin} directory within \texttt{HADOOP\_HOME}.
    Each variable is exported to be available in the user shell.
    This makes Hadoop usable from the login shell of the user.
    \item Now the configuration files of Hadoop are adjusted to the cluster needs. Each of those resides in the \texttt{etc/hadoop} directory within \texttt{HADOOP\_HOME}.
    \begin{itemize}
        \item \texttt{core-site.xml} -- This file describes the most basic settings for the instaled Hadoop instance. 
        Here the settings for the file system is set to use the \ac{FQDN} of the master to connect to.
        \item \texttt{hadoop-env.sh} -- This file describes all the environment variables used while running Hadoop. 
        Here \texttt{JAVA\_HOME} is set to the same value used above.
        \item \texttt{hdfs-site.xml} This file describes how \ac{HDFS} is configured. 
        The storage directory is set to a persistent folder
        and \ac{HDFS} is instructed to bind to the \ac{IP} address 0.0.0.0 
        which means the file system is accessible from all network interfaces.
        \item \texttt{mapred-site.xml} -- Here the settings for MapReduce are made. 
        It is instructed to use \ac{YARN} as the underlying framework.
        \item \texttt{slaves} -- This file is used on the master host to identify all the hosts 
        that run an nodemanager or an datanode.
        Therefore the \acs{FQDN} of the master and the salve host are entered here.
        \item \texttt{yarn-site.xml} -- This file describes the settings for \ac{YARN}. Here the master's \ac{FQDN} is entered as the resourcemanager and \ac{YARN} is bound to 0.0.0.0. 
        Furthermore the minimal allocation units for \ac{CPU} cores 
        as well as \ac{RAM} and their respective maximum allocation are defined. 
        This dictates how \ac{YARN} uses the resources of this host.
    \end{itemize}
\end{enumerate}

After the configuration is done for the master host, the same procedure can be followed for the slave. 
The only exceptions are that no entries are made in the \texttt{slaves} file 
and the \texttt{hdfs-site.xml} is configured, so that \ac{HDFS} uses the attached data drive to store file contents.

Finally the \ac{HDFS} file systems on each host can be formatted using the TODO WHICH COMMAND command.
Then \ac{YARN} and \ac{HDFS}, aswell as the MapReduce history server is started on the master, 
which in turn start the namenode and resoucemanager on the master 
as well as the datanoode and nodemanager on both the master and the slave.
Now the cluster can be used to store data in \ac{HDFS} and run MapReduce jobs.

\subsubsection{Tests}
\autocite[][]{white2015hadoop} provides an online reference\footnote{http://hadoopbook.com/} 
to test data and MapReduce programs 
that can be used for demonstration purposes of the Hadoop cluster.
The provided data includes weather data produced by the \ac{NCDC} and programs that can be run on Hadoop to analyze this data. 
Storing this data in \ac{HDFS} and running the MapReduce jobs lead to the discovery of erroneous configurations in the cluster.

Further test included the execution \emph{teragen} and \emph{terasort}, also provided by White, which generate a variable amount of numbers and sorts them, in this case 5 \ac{GB} of data. 
The generation  was successful, however the sorting failed due to an repeated connection timeout, which could not be resolved.

\subsubsection{Errors and Lessons Learned}
The whole described setup procedure is performed manually 
and therefore rather error prone.
Special attention to the usage of \acs{FQDN} must be given, 
so that the names used in Hadoop and the hostnames match up.
Those also need to be used in the \texttt{hosts} files on all hosts.

Furthermore it is important to configure the firewalls of OpenStack correctly, 
so that the nodes can be reached internally and externally.

Errors in the configuration lead to nodes that are not recognized by Hadoop or jobs that do not succeed.
It is rather frustrating to find and fix those issues for the person installing the cluster.

\subsubsection{Conclusions}

Due to the difficulties that the method of manually installing Hadoop on each host holds it is favorable to use a more streamlined 
and automated installation method as described in the next two sections.
This way a lot of manual configuration can be avoided.
Furthermore integrating tools such as Spark into the cluster 
will lead to even more complex installation and configuration processes which again might introduce new openings for errors.

\subsection{\acl{CDH}}

Cloudera distributes an Hadoop management system called \ac{CDH}.
The company provides an docker image which can be used to easily evaluate the usability of \ac{CDH}.
It contains \enquote{a single-host deployment of the Cloudera open-source distribution, including CDH and Cloudera Manager.} \autocite[][]{cloudera2018docker}
It does not provide a full cluster installation, but only a test environment to evaluate the plattform.

TODO explain what Cloudera distribution does


\subsubsection{System Setup}

As in the previous section, BW-Cloud will be used for installing a test setup.
Since the Docker image only runs on a single host, this host should be as powerfull as possible.
Therefore the largest available \ac{VM} preset is used, which consistes of 2 virtual \ac{CPU} cores, 
4 \ac{GB} \ac{RAM}, and 20 \ac{GB} storage. The operating is again Ubuntu 16.04 \ac{LTS}, 
and the host is accessible via the internet. 
Cloudera describes how Docker can be installed and the test image can be started, 
these instructions are followed. \autocite[][]{cloudera2018docker}
To do so, the Docker runtime is installed, a new  user \emph{docker} is created and added to the \emph{docker} group. To test the Docker installation, the \emph{hello-world} image can be started.
Now all ports required by \ac{CDH} need to be accesible through the OpenStack firewall.
Namely these are 8888, 7080, and 8080 (which is used to redirect to port 80 inside the container).
After fetching the Docker  image, it can be started in daemon mode with the necessary ports exposed using the command\\
\texttt{\$ docker run --hostname=quickstart.cloudera --privileged=true -t -i -d -p 8888 -p 7080 -p 8080:80 cloudera/quickstart /usr/bin/docker-quickstart}\\
All services that are needed inside the container are started automatically.


\subsubsection{Tests}
The Docker image includes an web based tutorial, 
that can be accessed on port 8080 (remapped before) on the host.
The tutorial highlights the features of \ac{CDH} which differ from other Hadoop distributions.
The tutorial describes a story from the perspective of an data analyst and shows how the features provide value for typical use cases.

\subsubsection{Conclusions}
The tutorial provides a zero-configuration hands-on experience which focuses on the features specific to Cloudera's Hadoop distribution.
The installation inside the docker container seems stable.
However it is not possible to assume that this implies an error free installation on an cluster, 
since possible obstacles are not foreseeable.

\subsection{Decision on Distribution}

TODO

At this point it is important to mention the project paper by Sicklinger et al., 
who did research on the topic of the possible usage of Hadoop management systems for the \ac{DHBW}.
Their findings suggest using Apache Ambari to deploy an Hadoop cluster at the \ac{DHBW}.
According to them, installing Hadoop manually is ruled out due to the technical complexity.
Apache Ambari and Cloudera are equally suitable for the usage at the \ac{DHBW}, 
but Ambari should be favoured since it is free to use and crates no licensing costs.
\autocite[][p. 53f]{wi2018managementsystems}

\subsection{\acl{HDP} with Ambari deployed with Ansible}
TODO

\subsubsection{System Setup}
TODO

\subsubsection{Tests}
TODO

\subsubsection{Conclusions}
TODO

\section{Architecture Design}
TODO

TODO

\section{Execution Plan}

TODO
